# LiteLLM Proxy Core Configuration
replicaCount: 1
image:
  repository: ghcr.io/berriai/litellm
  tag: v1.74.7-stable.patch.2

service:
  type: LoadBalancer
  port: 4000

# LiteLLM Proxy Specific Settings
litellmConfig:
  masterKey: "sk-1234"  # Rotate this in production!
  environment: "production"
  completionModel: "gpt-3.5-turbo"  # Default model
  allowedModels:  # Whitelist models
    - "gpt-3.5-turbo"
    - "gpt-4"
    - "claude-2"
    - "ollama"

# Resource Limits
resources:
  requests:
    cpu: "250m"
    memory: "512Mi"
  limits:
    cpu: "1"
    memory: "2Gi"